{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Approx nearest neighbor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN5oFJL3L6oQJughYarHUV4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukkatharun/ApproxNearestNeighborAlgos/blob/main/Approx_nearest_neighbor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsPPHRHTQY4o"
      },
      "source": [
        "Use state of art libraries for Approximate nearest neighbor search for your favorite data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II-4jPNSIjDM",
        "outputId": "bc1fd590-c240-4ab1-dba0-bd8c01bc0732"
      },
      "source": [
        "!pip install datasketch\n",
        "!pip install lightfm\n",
        "!pip install faiss-gpu\n",
        "!pip install nmslib\n",
        "!pip install annoy"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasketch in /usr/local/lib/python3.7/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from datasketch) (1.19.5)\n",
            "Requirement already satisfied: lightfm in /usr/local/lib/python3.7/dist-packages (1.16)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from lightfm) (2.23.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2021.10.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (1.1.0)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.7/dist-packages (1.7.1.post2)\n",
            "Requirement already satisfied: nmslib in /usr/local/lib/python3.7/dist-packages (2.1.1)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from nmslib) (1.19.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nmslib) (5.4.8)\n",
            "Requirement already satisfied: pybind11<2.6.2 in /usr/local/lib/python3.7/dist-packages (from nmslib) (2.6.1)\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
            "\u001b[K     |████████████████████████████████| 646 kB 5.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391671 sha256=660a31f5a9596a57ee28e524417ac3b6b55a961827bd4a180d3a7cd3cdab526c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMUtFptBfJCB"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from datasketch import MinHash, MinHashLSHForest\n",
        "import csv\n",
        "from lightfm import LightFM\n",
        "from lightfm.datasets import fetch_movielens\n",
        "import pickle\n",
        "import faiss\n",
        "import nmslib\n",
        "import annoy"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZeRfiTDNA7i"
      },
      "source": [
        "Locality sensitive hashing(LSH)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiCixmwcfY8s"
      },
      "source": [
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s\\n]','',text)\n",
        "    tokens = text.lower()\n",
        "    tokens = tokens.split()\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2H7exDYOgzx"
      },
      "source": [
        "permutations = 128\n",
        "num_recommendations = 1\n",
        "def get_forest(data, perms):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    minhash = []\n",
        "    \n",
        "    for text in data['text']:\n",
        "        tokens = preprocess(text)\n",
        "        m = MinHash(num_perm=perms)\n",
        "        for s in tokens:\n",
        "            m.update(s.encode('utf8'))\n",
        "        minhash.append(m)\n",
        "        \n",
        "    forest = MinHashLSHForest(num_perm=perms)\n",
        "    \n",
        "    for i,m in enumerate(minhash):\n",
        "        forest.add(i,m)\n",
        "        \n",
        "    forest.index()\n",
        "    \n",
        "    print('It took %s seconds to build forest.' %(time.time()-start_time))\n",
        "    \n",
        "    return forest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuLDIxAAOpPQ"
      },
      "source": [
        "def predict(text, database, perms, num_results, forest):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    tokens = preprocess(text)\n",
        "    m = MinHash(num_perm=perms)\n",
        "    for s in tokens:\n",
        "        m.update(s.encode('utf8'))\n",
        "        \n",
        "    idx_array = np.array(forest.query(m, num_results))\n",
        "    if len(idx_array) == 0:\n",
        "        return None # if your query is empty, return none\n",
        "    \n",
        "    result = database.iloc[idx_array]['title']\n",
        "    \n",
        "    print('It took %s seconds to query forest.' %(time.time()-start_time))\n",
        "    \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtALUegtOuZT",
        "outputId": "f1537b07-e3e0-4e59-f1f0-00be9090694e"
      },
      "source": [
        "db = pd.read_csv('papers.csv',error_bad_lines=False, engine=\"python\")\n",
        "db['text'] = db['title'] + ' ' + db['abstract']\n",
        "forest = get_forest(db, permutations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Skipping line 4574: unexpected end of data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It took 9.268283605575562 seconds to build forest.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmYn4OnhOyt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbbdedbe-9aed-438a-b52d-03cd5a450bf9"
      },
      "source": [
        "db.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4572, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "6CP2nRycR6D8",
        "outputId": "10b330eb-2825-490f-8f6e-f48a67a44c93"
      },
      "source": [
        "db.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id  ...                                               text\n",
              "0     1  ...  Self-Organization of Associative Database and ...\n",
              "1    10  ...  A Mean Field Theory of Layer IV of Visual Cort...\n",
              "2   100  ...  Storing Covariance by the Associative Long-Ter...\n",
              "3  1000  ...  Bayesian Query Construction for Neural Network...\n",
              "4  1001  ...  Neural Network Ensembles, Cross Validation, an...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_RslQI9R76r",
        "outputId": "00f84d55-af83-48d6-8050-c7b4992f2a72"
      },
      "source": [
        "num_recommendations = 5\n",
        "title = 'Bayesian Query Construction for Neural Network'\n",
        "result = predict(title, db, permutations, 5, forest)\n",
        "print('\\n Top Recommendation(s) is(are) \\n', result)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It took 0.004976511001586914 seconds to query forest.\n",
            "\n",
            " Top Recommendation(s) is(are) \n",
            " 3       Bayesian Query Construction for Neural Network...\n",
            "652     Global Optimisation of Neural Network Models v...\n",
            "308     Dynamically Adaptable CMOS Winner-Take-All Neu...\n",
            "859         Neural Network Based Model Predictive Control\n",
            "2047                         Neural Network Visualization\n",
            "Name: title, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yPEiddQNDz1"
      },
      "source": [
        "Product Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UXdBLnGSbdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28c8214-8281-4aa8-dcc7-ae65c3c572b2"
      },
      "source": [
        "from lightfm.datasets import fetch_movielens\n",
        "from lightfm.evaluation import precision_at_k\n",
        "from lightfm.evaluation import auc_score\n",
        "movielens = fetch_movielens()\n",
        "for key, value in movielens.items():\n",
        "    print(key, type(value), value.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train <class 'scipy.sparse.coo.coo_matrix'> (943, 1682)\n",
            "test <class 'scipy.sparse.coo.coo_matrix'> (943, 1682)\n",
            "item_features <class 'scipy.sparse.csr.csr_matrix'> (1682, 1682)\n",
            "item_feature_labels <class 'numpy.ndarray'> (1682,)\n",
            "item_labels <class 'numpy.ndarray'> (1682,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoNbYyUmOiL2",
        "outputId": "4ce7a126-85e9-4bdb-cc3c-2f19dde92fe3"
      },
      "source": [
        "train = movielens['train']\n",
        "test = movielens['test']\n",
        "\n",
        "model = LightFM(learning_rate=0.05, loss='bpr')\n",
        "model.fit(train, epochs=10)\n",
        "\n",
        "train_precision = precision_at_k(model, train, k=10).mean()\n",
        "test_precision = precision_at_k(model, test, k=10).mean()\n",
        "\n",
        "train_auc = auc_score(model, train).mean()\n",
        "test_auc = auc_score(model, test).mean()\n",
        "\n",
        "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
        "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: train 0.59, test 0.10.\n",
            "AUC: train 0.89, test 0.86.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewhT8MuuNMBr"
      },
      "source": [
        "train = movielens['train']\n",
        "test = movielens['test']\n",
        "item_vectors = movielens['item_features'] * model.item_embeddings"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCZtYSbmPKzD"
      },
      "source": [
        "with open('movielens.pickle', 'wb') as f:\n",
        "    pickle.dump({\"name\": movielens['item_feature_labels'], \"vector\": item_vectors}, f)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2cw8RVMNL-0",
        "outputId": "65abe124-bb97-4dbc-f7a6-6650288e9ecb"
      },
      "source": [
        "def load_data():\n",
        "    with open('movielens.pickle', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "data = load_data()\n",
        "data"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': array(['Toy Story (1995)', 'GoldenEye (1995)', 'Four Rooms (1995)', ...,\n",
              "        'Sliding Doors (1998)', 'You So Crazy (1994)',\n",
              "        'Scream of Stone (Schrei aus Stein) (1991)'], dtype=object),\n",
              " 'vector': array([[-2.8606594e-01,  2.3608355e-01,  3.6903396e-01, ...,\n",
              "          6.0274941e-01, -2.5291806e-01,  5.9300417e-01],\n",
              "        [ 4.4549662e-01,  6.2510759e-01,  2.2007335e-02, ...,\n",
              "          3.4654805e-01,  3.3281863e-01, -3.0071938e-01],\n",
              "        [ 1.5155029e-01,  4.0324977e-01,  4.5176700e-01, ...,\n",
              "          4.0110886e-01,  3.5056341e-01,  5.8581591e-01],\n",
              "        ...,\n",
              "        [-4.7731180e-02, -1.7034501e-01,  5.1332716e-02, ...,\n",
              "         -1.5277454e-01, -3.1650472e-02,  4.8078176e-02],\n",
              "        [-5.2377254e-02,  1.3624683e-01,  7.1398795e-02, ...,\n",
              "          8.5100338e-02,  7.0923008e-02, -5.3595174e-02],\n",
              "        [-1.9358748e-04,  1.2678677e-02, -8.8416830e-02, ...,\n",
              "          3.6688901e-02,  9.2964862e-03,  3.0345459e-02]], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frFosBQPNL8M"
      },
      "source": [
        "class ProductIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimension = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels    \n",
        "    def build(self, number_of_partition=8, search_in_x_partitions=2, subvector_size=8):\n",
        "        quantizer = faiss.IndexFlatL2(self.dimension)\n",
        "        self.index = faiss.IndexIVFPQ(quantizer, \n",
        "                                      self.dimension, \n",
        "                                      number_of_partition, \n",
        "                                      search_in_x_partitions, \n",
        "                                      subvector_size)\n",
        "        self.index.train(self.vectors)\n",
        "        self.index.add(self.vectors)\n",
        "        \n",
        "    def query(self, vectors, k=10):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        # I expect only query on one vector thus the slice\n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQNyImWDNL5Z"
      },
      "source": [
        "index = ProductIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBtynU5ONL1L",
        "outputId": "6cded5cc-c993-42f6-f225-95041e89a51e"
      },
      "source": [
        "movie_index = 90\n",
        "movie_vector = data['vector'][movie_index:movie_index+1]\n",
        "print(f\"The most simillar movie to {data['name'][movie_index]} are:\")\n",
        "index.query(movie_vector)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most simillar movie to Nightmare Before Christmas, The (1993) are:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Nightmare Before Christmas, The (1993)',\n",
              " 'Mask, The (1994)',\n",
              " 'Clueless (1995)',\n",
              " 'Sneakers (1992)',\n",
              " 'Three Musketeers, The (1993)',\n",
              " 'Princess Bride, The (1987)',\n",
              " 'Interview with the Vampire (1994)',\n",
              " 'Brady Bunch Movie, The (1995)',\n",
              " 'Aladdin (1992)',\n",
              " 'Conan the Barbarian (1981)']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeDbn1zvNF8G"
      },
      "source": [
        "Trees and Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aKlzK7MNNIP"
      },
      "source": [
        "class TreesIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "    def build(self, number_of_trees=5):\n",
        "        self.index = annoy.AnnoyIndex(self.dimention)\n",
        "        for i, vec in enumerate(self.vectors):\n",
        "            self.index.add_item(i, vec.tolist())\n",
        "        self.index.build(number_of_trees)\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        indices = self.index.get_nns_by_vector(vector.tolist(), k)\n",
        "        return [self.labels[i] for i in indices]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxeUO1v5NNGJ",
        "outputId": "9b0a8235-8c35-4d81-b609-170e448095b6"
      },
      "source": [
        "index = TreesIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDf-QLTYNM9y",
        "outputId": "b42fb5fd-c6dc-40c3-c36d-da7864c81976"
      },
      "source": [
        "movie_vector, movie_name = data['vector'][90], data['name'][90]\n",
        "similar_movie_questions = '\\n* '.join(index.query(movie_vector))\n",
        "print(f\"The most similar movie to {movie_name} are:\\n* {simlar_movie_questions}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar movie to Nightmare Before Christmas, The (1993) are:\n",
            "* Nightmare Before Christmas, The (1993)\n",
            "* Mask, The (1994)\n",
            "* Batman (1989)\n",
            "* Ready to Wear (Pret-A-Porter) (1994)\n",
            "* Princess Bride, The (1987)\n",
            "* Fatal Instinct (1993)\n",
            "* Private Benjamin (1980)\n",
            "* Interview with the Vampire (1994)\n",
            "* Heavy Metal (1981)\n",
            "* Hunt for Red October, The (1990)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdllC9j5NF5A"
      },
      "source": [
        "Hierarchical Navigable Small World Algorithm(HNSW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc19TNlkNK9t"
      },
      "source": [
        "class HNSWIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "    def build(self):\n",
        "        self.index = nmslib.init(method='hnsw', space='cosinesimil')\n",
        "        self.index.addDataPointBatch(self.vectors)\n",
        "        self.index.createIndex({'post': 2})\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        indices = self.index.knnQuery(vector, k=k)\n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE0xiUlNNN_-"
      },
      "source": [
        "index = HNSWIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeKO5G-gNN-M",
        "outputId": "2b131670-251a-4691-9ed5-b7a2868d2cb7"
      },
      "source": [
        "movie_vector, movie_name = data['vector'][90], data['name'][90]\n",
        "simlar_movie_questions = '\\n* '.join(index.query(movie_vector))\n",
        "print(f\"The most similar stack to {movie_name} are:\\n* {simlar_movie_questions}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar stack to Nightmare Before Christmas, The (1993) are:\n",
            "* Nightmare Before Christmas, The (1993)\n",
            "* Mask, The (1994)\n",
            "* Batman (1989)\n",
            "* Ready to Wear (Pret-A-Porter) (1994)\n",
            "* Princess Bride, The (1987)\n",
            "* Fatal Instinct (1993)\n",
            "* Private Benjamin (1980)\n",
            "* Interview with the Vampire (1994)\n",
            "* Heavy Metal (1981)\n",
            "* Hunt for Red October, The (1990)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZXHloEhPjkN"
      },
      "source": [
        "Exhaustive search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIFiLSxzPlBM"
      },
      "source": [
        "class ExhaustiveIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimension = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels    \n",
        "   \n",
        "    def build(self):\n",
        "        self.index = faiss.IndexFlatL2(self.dimension,)\n",
        "        self.index.add(self.vectors)\n",
        "        \n",
        "    def query(self, vectors, k=10):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdP4eLUvPlVJ"
      },
      "source": [
        "index = ExhaustiveIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylj3knEBPnSG",
        "outputId": "84e8fd24-4fe3-4272-dbfc-028b857b43dd"
      },
      "source": [
        "movie_vector, movie_name = data['vector'][90:91], data['name'][90]\n",
        "simlar_movie_questions = '\\n* '.join(index.query(movie_vector))\n",
        "print(f\"The most similar movie to {movie_name} are:\\n* {simlar_movie_questions}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar movie to Nightmare Before Christmas, The (1993) are:\n",
            "* Nightmare Before Christmas, The (1993)\n",
            "* Mask, The (1994)\n",
            "* Princess Bride, The (1987)\n",
            "* Heavy Metal (1981)\n",
            "* Interview with the Vampire (1994)\n",
            "* Clueless (1995)\n",
            "* Hunt for Red October, The (1990)\n",
            "* Sneakers (1992)\n",
            "* Better Off Dead... (1985)\n",
            "* Grease (1978)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVXKFxrkPpLx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}